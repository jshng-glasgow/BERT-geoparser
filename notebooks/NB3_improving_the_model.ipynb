{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "# third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# local imports\n",
    "from BERT_geoparser.data import Data, Phrase\n",
    "from BERT_geoparser.tokenizer import Tokenizer\n",
    "from BERT_geoparser.model import BertModel\n",
    "from BERT_geoparser.analysis import Results\n",
    "from BERT_geoparser.retagger import Retagger\n",
    "from BERT_geoparser.utils import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on randomized locations\n",
    "While these results are promising, there is a serious flaw in the datset the model has been trained on. Specifically, the reviews are from businesses from a small number of locations. As such, there is a danger that the model is simply consistently assigning the `tar` tag to tokens matching those places. To investigate we will build a new dataset with all the tokens tagged as `tar` replaced with random draws from a list of place names. \n",
    "\n",
    "One approach to doing this would be to go through the test data and replace anything tagged as a target location with a new location. However, this will cause problems with indexing if the token representation of the new location is longer or shorter than the previous location (e.g. ['New', 'York'] -> ['LA']). \n",
    "\n",
    "To get around any problems with indexing we willgenerate a new reviews dataset, with the text and coordinates altered when a place tagged as `tar` is mentioned. We will then go through the process of NER tagging and `tar`/`inc` tagging again to produce a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the test data\n",
    "test_data = pd.read_csv('data/step_4/test_yelp_dataset.csv')\n",
    "# extract only lines tagged as target\n",
    "target_only_data = test_data[test_data.Tag.str.contains('tar')]\n",
    "# use the Retagger class to add a 'sequential group' column to this data.\n",
    "retagger = Retagger(target_only_data)\n",
    "retagger.add_sequential_groups()\n",
    "retagger.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset of world cities, and produce a list of US cities and a (lat,long) coordinate for that city. This will be important for re-tagging the dataset as `tar` and `inc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_cities_df = pd.read_csv('data/model_improvement/worldcities.csv')\n",
    "us_cities_df = world_cities_df[world_cities_df.iso3=='USA']\n",
    "us_cities = []\n",
    "for i, city in us_cities_df.iterrows():\n",
    "    name = city.city_ascii\n",
    "    lat = city.lat\n",
    "    lng = city.lng\n",
    "    us_cities.append({'name':name, 'coords':str((lat,lng))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to loop over the test locations in the test dataset (i.e. groups of sequentially indexed tokens tagged as `tar`) and create a dictionary which maps the review number (`Sentence #`) to a copy of the review text with the location replaced with a random draw from the cities dataset and the set of coordinates related to the new location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('data/step_2/25k_yelp_reviews_with_location.csv', nrows=25000)[20000:]\n",
    "review_df = review_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.loc[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want the word 'Philly' to be replaced with another random city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for replacements, mapping a review index to a new  \n",
    "replacement_dict = {}\n",
    "\n",
    "for i, group in retagger.df.groupby('sequential_group'):\n",
    "    # build the phrase for this group of words\n",
    "    old_name = Phrase('', tag=None)\n",
    "    for token, tag in zip(group['Word'].values, group['Tag'].values):\n",
    "        old_name.add_token(token=token, tag=tag)\n",
    "    # get a new city and coordinate pair\n",
    "    new_city = random.choice(us_cities)\n",
    "    # get the old text and replace the city name\n",
    "    review_num = group['Sentence #'].iloc[0]\n",
    "    old_text = review_df.loc[review_num].text.lower()\n",
    "    new_text = old_text.replace(old_name.text, new_city['name'])\n",
    "    # update the review dataframe\n",
    "    review_df.loc[review_num, 'text'] = new_text\n",
    "    review_df.loc[review_num, 'coordinates'] = new_city['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.loc[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked! We can now build a new test datset out of this randomized data and see if the model is able to acheive an acceptable degree of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NER Tagging ###\n",
    "data_csv = r'../data/ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "model.model.summary()\n",
    "\n",
    "results = model.results_dataframe(texts=review_df.text.values, include_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tar/inc tagging\n",
    "retagger = Retagger(results)\n",
    "retagger.retag(['geo', 'gpe', 'org'], threshold='bbox', review_df=review_df)\n",
    "retagged_data = retagger.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retagged_data[retagged_data['Sentence #']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retagged_data.to_csv('data/step_4/test_yelp_dataset_randomized_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = 'data/step_4/test_yelp_dataset_randomized_locations.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "tar_model = BertModel(saved_model='20230926_tar_tagged_bert_model_large.hdf5', data=data)\n",
    "X_tokens, y_pred, y_true = tar_model.test('data/step_4/test_yelp_dataset_randomized_locations.csv', return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'tar', 'inc']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 3)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),3)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not good. The model clearly struggles with the randomized location data, indicating that the high accuracy acheived on the previous dataset was likely a product of the limited location, rather than any understanding of sentence structure. \n",
    "\n",
    "## Re-train the model on randomized location data\n",
    "The first approach to this should be to retrain the model on the new data. This will help the model generalise a little better about locations, but may result in some of the finer nuance in the data being lost. Contextual information, such as foods, activities or sights specific to a particular location, will be lost. This highlights one of the key limitations of the yelp data. With data which is already more generalised this contextualisation might be possible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a new training set\n",
    "# open the test data\n",
    "train_data = pd.read_csv('data/step_4/test_yelp_dataset.csv')\n",
    "# extract only lines tagged as target\n",
    "target_only_data = train_data[train_data.Tag.str.contains('tar')]\n",
    "# use the Retagger class to add a 'sequential group' column to this data.\n",
    "retagger = Retagger(target_only_data)\n",
    "retagger.add_sequential_groups()\n",
    "retagger.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('data/step_2/25k_yelp_reviews_with_location.csv', nrows=25000)\n",
    "review_df = review_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for replacements, mapping a review index to a new  \n",
    "replacement_dict = {}\n",
    "\n",
    "for i, group in retagger.df.groupby('sequential_group'):\n",
    "    # build the phrase for this group of words\n",
    "    old_name = Phrase('', tag=None)\n",
    "    for token, tag in zip(group['Word'].values, group['Tag'].values):\n",
    "        old_name.add_token(token=token, tag=tag)\n",
    "    # get a new city and coordinate pair\n",
    "    new_city = random.choice(us_cities)\n",
    "    # get the old text and replace the city name\n",
    "    review_num = group['Sentence #'].iloc[0]\n",
    "    old_text = review_df.loc[review_num].text.lower()\n",
    "    new_text = old_text.replace(old_name.text, new_city['name'])\n",
    "    # update the review dataframe\n",
    "    review_df.loc[review_num, 'text'] = new_text\n",
    "    review_df.loc[review_num, 'coordinates'] = new_city['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NER Tagging ###\n",
    "data_csv = r'../data/ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "model.model.summary()\n",
    "\n",
    "results = model.results_dataframe(texts=review_df.text.values, include_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tar/inc tagging\n",
    "retagger = Retagger(results)\n",
    "retagger.retag(['geo', 'gpe', 'org'], threshold='bbox', review_df=review_df)\n",
    "retagged_data = retagger.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retagged_data.to_csv('data/model_improvement/randomised_location_tarinc_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NER Tagging ###\n",
    "data_csv = r'data/model_improvement/randomised_location_tarinc_train.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model=None, data=data)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "data = pd.read_csv(data_csv)\n",
    "class_weights_list = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes=['B-inc', 'B-tar', 'I-inc', 'I-tar', 'O'],\n",
    "                                                 y=data.Tag.values)\n",
    "\n",
    "class_weights = {i:w for i,w in enumerate(class_weights_list)}\n",
    "class_weights.update({5:0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(save_as='20230929_tar_model_randomised_locations.hdf5', \n",
    "            n_epochs=2,\n",
    "            batch_size=16, \n",
    "            validation_split=0.1, \n",
    "            class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = 'data/step_4/test_yelp_dataset_randomized_locations.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "tar_model = BertModel(saved_model='20230929_tar_model_randomised_locations.hdf5', data=data)\n",
    "X_tokens, y_pred, y_true = tar_model.test('data/step_4/test_yelp_dataset_randomized_locations.csv', return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'tar', 'inc']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 3)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),3)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights_list = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes=np.unique(retagged_data.Tag),\n",
    "                                                 y=retagged_data.Tag.values)\n",
    "\n",
    "class_weights = {i:w for i,w in enumerate(class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(save_as='20230929_tar_model_randomised_locations.hdf5', n_epochs=1, batch_size=16, validation_split=0.1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
