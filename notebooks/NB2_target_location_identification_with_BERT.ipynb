{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import sys\n",
    "sys.path.append('../BERT_geoparser/')\n",
    "# third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from shapely.geometry import box, Point\n",
    "from tqdm import tqdm\n",
    "# local imports\n",
    "from data import Data, Phrase\n",
    "from tokenizer import Tokenizer\n",
    "from model import BertModel\n",
    "from analysis import Results\n",
    "from retagger import Retagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: building a target location identification model\n",
    "This notebook goes through the steps neccessary for using a trained NER-tagging model (as built in notebook 1) to build a target/incidental location BERT model. This will involve 3 steps:\n",
    "\n",
    "1. Open the yelp review dataset and link each review to the geographic location of the business.\n",
    "2. Use a trained BERT model to NER tag the text in each review.\n",
    "3. Retag the data by assessing the proximity of any tokens (or strings of tokens) tagged as locations to the true location identified in step 1.\n",
    "4. Use this re-tagged data to retrain the BERT-model on the task of identifying target and incidental locations mentioned in text.\n",
    "\n",
    "By doing this, the goal is to produce a model which can parse locations in text in a sensible way - identifying locations which the text is directly referencing ('target' locations) and other locations which are not related to the business being reviewed ('incidental' locations). For example,  the sentence:\n",
    "\n",
    " <p style=\"text-align: center;\"> \"Donnies Pizza Heaven in <span style=\"color:green\">Inidanapolis</span> do the best deep pan pizza outside of <span style=\"color:red\">Chicago</span>.\" </p>\n",
    "\n",
    " would receive the tags:\n",
    "\n",
    "  <p style=\"text-align: center;\"> \"[O] [O] [O] [O] <span style=\"color:green\">[B-tar]</span> [O] [O] ... <span style=\"color:red\">[B-inc]</span>.\" </p>\n",
    "\n",
    "  ## Step 1: adding locations to review data\n",
    "  The first step is to add the locations to the yelp review data. We can do this by linking the `yelp_academic_dataset_review.json` data with the `yelp_academic_dataset_business.json` data using the `business_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the review dataset and the accompanying business info dataset\n",
    "review_df = pd.read_json('../data/yelp_academic_dataset_review.json', lines=True, nrows=20000)#[10000:]\n",
    "business_df = pd.read_json('../data/yelp_academic_dataset_business.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [07:56<00:00, 42.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# add locations to reviews by linking to business\n",
    "def get_coords(business_ids):\n",
    "    coords = []\n",
    "    for id in tqdm(business_ids):\n",
    "        business = business_df[business_df.business_id==id]\n",
    "        lat = business.latitude.iloc[0]\n",
    "        lon = business.longitude.iloc[0]\n",
    "        coords.append((lat, lon))\n",
    "    return coords\n",
    "\n",
    "review_df['coordinates'] = get_coords(review_df.business_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.to_csv('data/step_2/yelp_reviews_with_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: parsing the reviews using the BERT model\n",
    "We will load a pre-trained BERT model using the `Data`, `Tokenizer` and `BERTModel` classes and use this to parse the text in the review dataset. This will output a dataframe set up in a way that makes it easy to use to re-train a new model on in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 125,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 125, 768)     0           ['tf_bert_model_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 125, 18)      13842       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,496,082\n",
      "Trainable params: 109,496,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using the BERT_geoparser Data.py module\n",
    "data_csv = r'../data/ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [29:25<00:00, 11.33it/s]\n"
     ]
    }
   ],
   "source": [
    "results = model.results_dataframe(texts=review_df.text.values, include_best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: re-tagging the reviews with target/incidental locations\n",
    "We can now use the `Retagger` class to tag every location identified in step 2 with a new 'target' or 'incidental' tag. We will consider locations as being anything with a tag containing either `geo`, `org` or `gpe`. Everything elase will be given an `O` tag. \n",
    "\n",
    "We will complete proximity checks using a bounding box around the matched locations. This is acheived by setting `threshold='bbox'`. This will mean that any tokens or phrase for which the true location is within the bounding box of any locations matched to the word or phrase will be tagged with `tar` and other will be tagged `inc`. Alternatively, we could set a minimum distance (in KM) for these proximity checks by setting `threshold=<float>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jshin\\Documents\\Work\\DSO\\BERT_geoparser\\BERT_geoparser\\retagger.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df.loc[:, 'sequential_group'] = groups\n",
      "100%|██████████| 23365/23365 [3:18:28<00:00,  1.96it/s]  \n"
     ]
    }
   ],
   "source": [
    "retagger = Retagger(results)\n",
    "retagger.retag(['geo', 'gpe', 'org'], threshold='bbox', review_df=review_df)\n",
    "retagged_data = retagger.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retagged_data.to_csv('data/step_4/test_yelp_tar_inc_tagged_bbx_org_geo_gpe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tags : 1695422\n",
      "B-tar tags : 4866\n",
      "I-tar tags : 1460\n",
      "B-inc tags : 26075\n",
      "I-inc tags : 14763\n"
     ]
    }
   ],
   "source": [
    "# get some info about distribution of targets\n",
    "O = retagged_data[retagged_data.Tag == 'O']\n",
    "B_inc = retagged_data[retagged_data.Tag == 'B-inc']\n",
    "B_tar = retagged_data[retagged_data.Tag == 'B-tar']\n",
    "I_inc = retagged_data[retagged_data.Tag == 'I-inc']\n",
    "I_tar = retagged_data[retagged_data.Tag == 'I-tar']\n",
    "\n",
    "print(f'O tags : {len(O)}')\n",
    "print(f'B-tar tags : {len(B_tar)}')\n",
    "print(f'I-tar tags : {len(I_tar)}')\n",
    "print(f'B-inc tags : {len(B_inc)}')\n",
    "print(f'I-inc tags : {len(I_inc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrain a new BERT-model on the new tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_4[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_6[0][0]',                \n",
      "                                tentions(last_hidde               'input_5[0][0]']                \n",
      "                                n_state=(None, 125,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_112 (Dropout)          (None, 125, 768)     0           ['tf_bert_model_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 125, 6)       4614        ['dropout_112[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,854\n",
      "Trainable params: 109,486,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jshin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_path = r'data/step_4/test_yelp_tar_inc_tagged_bbx_org_geo_gpe.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_path, tokenizer=tokenizer, max_len=125)\n",
    "tar_model = BertModel(saved_model=False, data=data)\n",
    "tar_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1125/1125 [==============================] - 242s 208ms/step - loss: 0.0556 - accuracy: 0.7231 - val_loss: 0.0364 - val_accuracy: 0.7224\n",
      "Epoch 2/4\n",
      "1125/1125 [==============================] - 237s 210ms/step - loss: 0.0299 - accuracy: 0.7312 - val_loss: 0.0352 - val_accuracy: 0.7239\n",
      "Epoch 3/4\n",
      "1125/1125 [==============================] - 232s 206ms/step - loss: 0.0213 - accuracy: 0.7348 - val_loss: 0.0381 - val_accuracy: 0.7235\n",
      "Epoch 4/4\n",
      "1125/1125 [==============================] - 234s 208ms/step - loss: 0.0166 - accuracy: 0.7368 - val_loss: 0.0410 - val_accuracy: 0.7236\n"
     ]
    }
   ],
   "source": [
    "tar_model.train(save_as='20230926_tar_tagged_bert_model_large.hdf5', n_epochs=4, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the review dataset and the accompanying business info dataset\n",
    "review_df = pd.read_json('../data/yelp_academic_dataset_review.json', lines=True, nrows=25000)[20000:]\n",
    "business_df = pd.read_json('../data/yelp_academic_dataset_business.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:53<00:00, 43.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# add locations to reviews by linking to business\n",
    "def get_coords(business_ids):\n",
    "    coords = []\n",
    "    for id in tqdm(business_ids):\n",
    "        business = business_df[business_df.business_id==id]\n",
    "        lat = business.latitude.iloc[0]\n",
    "        lon = business.longitude.iloc[0]\n",
    "        coords.append((lat, lon))\n",
    "    return coords\n",
    "\n",
    "review_df['coordinates'] = get_coords(review_df.business_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [07:45<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using the BERT_geoparser Data.py module\n",
    "data_csv = r'../data/ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "results = model.results_dataframe(texts=review_df.text.values, include_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jshin\\Documents\\Work\\DSO\\BERT_geoparser\\BERT_geoparser\\retagger.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  groups.append(current_group)\n",
      "100%|██████████| 5800/5800 [48:59<00:00,  1.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "retagger = Retagger(results)\n",
    "review_df = review_df.reset_index()\n",
    "retagger.retag(['geo', 'gpe', 'org'], threshold='bbox', review_df=review_df)\n",
    "test_data = retagger.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('data/step_4/test_yelp_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 25s 135ms/step\n"
     ]
    }
   ],
   "source": [
    "data_csv = 'data/step_4/test_yelp_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "tar_model = BertModel(saved_model='20230926_tar_tagged_bert_model_large.hdf5', data=data)\n",
    "X_tokens, y_pred, y_true = tar_model.test('data/step_4/test_yelp_dataset.csv', return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"O\" accuracy : 0.994\n",
      "\"O\" precision : 0.991\n",
      "\"O\" recall : 0.994\n",
      "=======================\n",
      "\"tar\" accuracy : 0.799\n",
      "\"tar\" precision : 0.822\n",
      "\"tar\" recall : 0.802\n",
      "=======================\n",
      "\"inc\" accuracy : 0.679\n",
      "\"inc\" precision : 0.773\n",
      "\"inc\" recall : 0.691\n",
      "=======================\n",
      "macro average recall : 0.733\n",
      "macro average precision : 0.826\n",
      "micro average recall : 0.983\n",
      "micro average precision : 0.983\n"
     ]
    }
   ],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'tar', 'inc']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 3)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),3)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = {'Sentence #': [], 'Word':[], 'Tag':[], 'Predicted Tag':[]}\n",
    "for i, (X, y, y_hat) in enumerate(zip(X_tokens, y_true, y_pred)):\n",
    "    rows['Sentence #'].extend([i]*len(X))\n",
    "    rows['Word'].extend(X)\n",
    "    rows['Tag'].extend(y)\n",
    "    rows['Predicted Tag'].extend(y_hat)\n",
    "results_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('data/predictions/20230927_tar_inc_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model improvement\n",
    "While these results are promising, there is a serious flaw in the datset the model has been trained on. Specifically, the reviews are from businesses from a small number of locations. As such, there is a danger that the model is simply consistently assigning the `tar` tag to tokens matching those places. To investigate we will build a new dataset with all the tokens tagged as `tar` replaced with random draws from a list of place names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jshin\\Documents\\Work\\DSO\\BERT_geoparser\\BERT_geoparser\\retagger.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df.loc[:, 'sequential_group'] = groups\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>B-art</th>\n",
       "      <th>B-eve</th>\n",
       "      <th>B-geo</th>\n",
       "      <th>B-gpe</th>\n",
       "      <th>B-nat</th>\n",
       "      <th>B-org</th>\n",
       "      <th>B-per</th>\n",
       "      <th>...</th>\n",
       "      <th>I-geo</th>\n",
       "      <th>I-gpe</th>\n",
       "      <th>I-nat</th>\n",
       "      <th>I-org</th>\n",
       "      <th>I-per</th>\n",
       "      <th>I-tim</th>\n",
       "      <th>O</th>\n",
       "      <th>old_tag</th>\n",
       "      <th>Tag</th>\n",
       "      <th>sequential_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>phil</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>B-tar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>##ly</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>B-tar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>897</td>\n",
       "      <td>9</td>\n",
       "      <td>shell</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.041</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>B-tar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>898</td>\n",
       "      <td>9</td>\n",
       "      <td>key</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.027</td>\n",
       "      <td>I-geo</td>\n",
       "      <td>I-tar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>899</td>\n",
       "      <td>9</td>\n",
       "      <td>island</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.084</td>\n",
       "      <td>I-geo</td>\n",
       "      <td>I-tar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Sentence #    Word  B-art  B-eve  B-geo  B-gpe  B-nat  B-org  \\\n",
       "66      66           0    phil  0.000  0.000  0.981  0.001  0.000  0.007   \n",
       "67      67           0    ##ly  0.000  0.000  0.987  0.001  0.000  0.006   \n",
       "897    897           9   shell  0.037  0.003  0.469  0.002  0.001  0.358   \n",
       "898    898           9     key  0.004  0.002  0.061  0.001  0.001  0.004   \n",
       "899    899           9  island  0.002  0.000  0.011  0.001  0.000  0.002   \n",
       "\n",
       "     B-per  ...  I-geo  I-gpe  I-nat  I-org  I-per  I-tim      O  old_tag  \\\n",
       "66   0.000  ...  0.001  0.000  0.000  0.000  0.000  0.000  0.005    B-geo   \n",
       "67   0.000  ...  0.002  0.000  0.000  0.000  0.000  0.000  0.002    B-geo   \n",
       "897  0.047  ...  0.011  0.000  0.001  0.015  0.002  0.002  0.041    B-geo   \n",
       "898  0.009  ...  0.583  0.003  0.001  0.212  0.048  0.004  0.027    I-geo   \n",
       "899  0.002  ...  0.619  0.002  0.001  0.218  0.023  0.003  0.084    I-geo   \n",
       "\n",
       "       Tag  sequential_group  \n",
       "66   B-tar                 0  \n",
       "67   B-tar                 0  \n",
       "897  B-tar                 1  \n",
       "898  I-tar                 1  \n",
       "899  I-tar                 1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the test data\n",
    "test_data = pd.read_csv('data/step_4/test_yelp_dataset.csv')\n",
    "# extract only lines tagged as target\n",
    "target_only_data = test_data[test_data.Tag.str.contains('tar')]\n",
    "# use the Retagger class to add a 'sequential group' column to this data.\n",
    "retagger = Retagger(target_only_data)\n",
    "retagger.add_sequential_groups()\n",
    "retagger.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from BERT_geoparser.utils import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ids = np.asarray(flatten(data.build_input_from_text('Penrith')[0]))\n",
    "new_ids = new_ids[~np.isin(new_ids, (101,102,0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jshin\\AppData\\Local\\Temp\\ipykernel_32144\\3847452423.py:1: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  flatten(data.build_input_from_text('Penrith')[0])[~np.isin(101,102,0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7279"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(data.build_input_from_text('Penrith')[0])[~np.isin(101,102,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pen', '##rith']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data.tokenizer.id_to_token(i) for i in new_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_cities_df = pd.read_csv('data/model_improvement/worldcities.csv')\n",
    "us_cities_df = world_cities_df[world_cities_df.iso3=='USA']\n",
    "us_cities = []\n",
    "for i, city in us_cities_df.iterrows():\n",
    "    name = city.city_ascii\n",
    "    lat = city.lat\n",
    "    lng = city.lng\n",
    "    us_cities.append((name, (lng,lat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {}\n",
    "\n",
    "for i, group in retagger.df.groupby('sequential_group'):\n",
    "    phrase = Phrase('', tag=None)\n",
    "    for token, tag in zip(group['Word'].values, group['Tag'].values):\n",
    "        phrase.add_token(token=token, tag=tag)\n",
    "    if phrase.text not in replacement_dict.keys():\n",
    "        new_cities = random.choices(us_cities, k=2)\n",
    "        # make sure we're not replacing with the same city\n",
    "        new_city = new_cities[0]\n",
    "        if new_city == phrase.text:\n",
    "            new_city == new_cities[1]\n",
    "        replacement_dict.update({phrase.text:new_city})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the review dataset and the accompanying business info dataset\n",
    "review_df = pd.read_json('../data/yelp_academic_dataset_review.json', lines=True, nrows=25000)[20000:]\n",
    "business_df = pd.read_json('../data/yelp_academic_dataset_business.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>B-art</th>\n",
       "      <th>B-eve</th>\n",
       "      <th>B-geo</th>\n",
       "      <th>B-gpe</th>\n",
       "      <th>B-nat</th>\n",
       "      <th>B-org</th>\n",
       "      <th>B-per</th>\n",
       "      <th>B-tim</th>\n",
       "      <th>I-art</th>\n",
       "      <th>I-eve</th>\n",
       "      <th>I-geo</th>\n",
       "      <th>I-gpe</th>\n",
       "      <th>I-nat</th>\n",
       "      <th>I-org</th>\n",
       "      <th>I-per</th>\n",
       "      <th>I-tim</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>if</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>you</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>decide</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>to</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855018</th>\n",
       "      <td>19996</td>\n",
       "      <td>out</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855019</th>\n",
       "      <td>19996</td>\n",
       "      <td>things</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855020</th>\n",
       "      <td>19996</td>\n",
       "      <td>on</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855021</th>\n",
       "      <td>19996</td>\n",
       "      <td>your</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855022</th>\n",
       "      <td>19996</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>855023 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #    Word  B-art  B-eve  B-geo  B-gpe  B-nat  B-org  B-per  \\\n",
       "0                0   [CLS]    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1                0      if    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2                0     you    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3                0  decide    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4                0      to    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...            ...     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "855018       19996     out    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "855019       19996  things    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "855020       19996      on    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "855021       19996    your    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "855022       19996   [SEP]    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "        B-tim  I-art  I-eve  I-geo  I-gpe  I-nat  I-org  I-per  I-tim    O  \n",
       "0         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "1         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "2         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "3         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "4         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...  ...  \n",
       "855018    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "855019    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "855020    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "855021    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "855022    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  1.0  \n",
       "\n",
       "[855023 rows x 19 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
