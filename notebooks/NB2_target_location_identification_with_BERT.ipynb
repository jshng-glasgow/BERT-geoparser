{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# local imports\n",
    "from BERT_geoparser.data import Data, Phrase\n",
    "from BERT_geoparser.tokenizer import Tokenizer\n",
    "from BERT_geoparser.model import BertModel\n",
    "from BERT_geoparser.analysis import Results\n",
    "from BERT_geoparser.retagger import Retagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: building a target location identification model\n",
    "This notebook goes through the steps neccessary for using a trained NER-tagging model (as built in notebook 1) to build a target/incidental location BERT model. This will involve 3 steps:\n",
    "\n",
    "1. Open the yelp review dataset and link each review to the geographic location of the business.\n",
    "2. Use a trained BERT model to NER tag the text in each review.\n",
    "3. Retag the data by assessing the proximity of any tokens (or strings of tokens) tagged as locations to the true location identified in step 1.\n",
    "4. Use this re-tagged data to retrain the BERT-model on the task of identifying target and incidental locations mentioned in text.\n",
    "\n",
    "By doing this, the goal is to produce a model which can parse locations in text in a sensible way - identifying locations which the text is directly referencing ('target' locations) and other locations which are not related to the business being reviewed ('incidental' locations). For example,  the sentence:\n",
    "\n",
    " <p style=\"text-align: center;\"> \"Donnies Pizza Heaven in <span style=\"color:green\">Inidanapolis</span> do the best deep pan pizza outside of <span style=\"color:red\">Chicago</span>.\" </p>\n",
    "\n",
    " would receive the tags:\n",
    "\n",
    "  <p style=\"text-align: center;\"> \"[O] [O] [O] [O] <span style=\"color:green\">[B-tar]</span> [O] [O] ... <span style=\"color:red\">[B-inc]</span>.\" </p>\n",
    "\n",
    "  ## Step 1: adding locations to review data\n",
    "  The first step is to add the locations to the yelp review data. We can do this by linking the `yelp_academic_dataset_review.json` data with the `yelp_academic_dataset_business.json` data using the `business_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the review dataset and the accompanying business info dataset\n",
    "review_df = pd.read_json('../data/yelp_academic_dataset_review.json', lines=True, nrows=25000)#[10000:]\n",
    "business_df = pd.read_json('../data/yelp_academic_dataset_business.json', lines=True)\n",
    "#review_df = pd.read_csv('data/model_improvement/randomised_review_locations.csv', nrows=1000)#[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [08:56<00:00, 46.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# add locations to reviews by linking to business\n",
    "def get_coords(business_ids):\n",
    "    coords = []\n",
    "    for id in tqdm(business_ids):\n",
    "        business = business_df[business_df.business_id==id]\n",
    "        lat = business.latitude.iloc[0]\n",
    "        lon = business.longitude.iloc[0]\n",
    "        coords.append((lat, lon))\n",
    "    return coords\n",
    "\n",
    "review_df['coordinates'] = get_coords(review_df.business_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.to_csv('data/step_2/25k_yelp_reviews_with_location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: parsing the reviews using the BERT model\n",
    "We will load a pre-trained BERT model using the `Data`, `Tokenizer` and `BERTModel` classes and use this to parse the text in the review dataset. This will output a dataframe set up in a way that makes it easy to use to re-train a new model on in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 125,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 125, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 125, 18)      13842       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,496,082\n",
      "Trainable params: 109,496,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using the BERT_geoparser Data.py module\n",
    "data_csv = r'../data/ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:25<00:00, 11.67it/s]\n"
     ]
    }
   ],
   "source": [
    "results = model.results_dataframe(texts=review_df.text.values, include_best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: re-tagging the reviews with target/incidental locations\n",
    "We can now use the `Retagger` class to tag every location identified in step 2 with a new 'target' or 'incidental' tag. We will consider locations as being anything with a tag containing either `geo`, `org` or `gpe`. Everything elase will be given an `O` tag. \n",
    "\n",
    "We will complete proximity checks using a bounding box around the matched locations. This is acheived by setting `threshold='bbox'`. This will mean that any tokens or phrase for which the true location is within the bounding box of any locations matched to the word or phrase will be tagged with `tar` and other will be tagged `inc`. Alternatively, we could set a minimum distance (in KM) for these proximity checks by setting `threshold=<float>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jshin\\Documents\\Work\\DSO\\BERT_geoparser\\BERT_geoparser\\retagger.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df.loc[:, 'sequential_group'] = groups\n",
      "100%|██████████| 1286/1286 [10:51<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "retagger = Retagger(results)\n",
    "retagger.retag(['geo', 'gpe', 'org'], threshold='bbox', review_df=review_df)\n",
    "retagged_data = retagger.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retagged_data.to_csv('data/step_4/test_data_random_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tags : 1695422\n",
      "B-tar tags : 4866\n",
      "I-tar tags : 1460\n",
      "B-inc tags : 26075\n",
      "I-inc tags : 14763\n"
     ]
    }
   ],
   "source": [
    "# get some info about distribution of targets\n",
    "O = retagged_data[retagged_data.Tag == 'O']\n",
    "B_inc = retagged_data[retagged_data.Tag == 'B-inc']\n",
    "B_tar = retagged_data[retagged_data.Tag == 'B-tar']\n",
    "I_inc = retagged_data[retagged_data.Tag == 'I-inc']\n",
    "I_tar = retagged_data[retagged_data.Tag == 'I-tar']\n",
    "\n",
    "print(f'O tags : {len(O)}')\n",
    "print(f'B-tar tags : {len(B_tar)}')\n",
    "print(f'I-tar tags : {len(I_tar)}')\n",
    "print(f'B-inc tags : {len(B_inc)}')\n",
    "print(f'I-inc tags : {len(I_inc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrain a new BERT-model on the new tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 125,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 125, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 125, 6)       4614        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,486,854\n",
      "Trainable params: 109,486,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jshin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_path = r'data/step_4/test_yelp_tar_inc_tagged_bbx_org_geo_gpe.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_path, tokenizer=tokenizer, max_len=125)\n",
    "tar_model = BertModel(saved_model=False, data=data)\n",
    "tar_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1125/1125 [==============================] - 242s 208ms/step - loss: 0.0556 - accuracy: 0.7231 - val_loss: 0.0364 - val_accuracy: 0.7224\n",
      "Epoch 2/4\n",
      "1125/1125 [==============================] - 237s 210ms/step - loss: 0.0299 - accuracy: 0.7312 - val_loss: 0.0352 - val_accuracy: 0.7239\n",
      "Epoch 3/4\n",
      "1125/1125 [==============================] - 232s 206ms/step - loss: 0.0213 - accuracy: 0.7348 - val_loss: 0.0381 - val_accuracy: 0.7235\n",
      "Epoch 4/4\n",
      "1125/1125 [==============================] - 234s 208ms/step - loss: 0.0166 - accuracy: 0.7368 - val_loss: 0.0410 - val_accuracy: 0.7236\n"
     ]
    }
   ],
   "source": [
    "tar_model.train(save_as='20230926_tar_tagged_bert_model_large.hdf5', n_epochs=4, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the review dataset and the accompanying business info dataset\n",
    "review_df = pd.read_json('../data/yelp_academic_dataset_review.json', lines=True, nrows=25000)[20000:]\n",
    "business_df = pd.read_json('../data/yelp_academic_dataset_business.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:53<00:00, 43.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# add locations to reviews by linking to business\n",
    "def get_coords(business_ids):\n",
    "    coords = []\n",
    "    for id in tqdm(business_ids):\n",
    "        business = business_df[business_df.business_id==id]\n",
    "        lat = business.latitude.iloc[0]\n",
    "        lon = business.longitude.iloc[0]\n",
    "        coords.append((lat, lon))\n",
    "    return coords\n",
    "\n",
    "review_df['coordinates'] = get_coords(review_df.business_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [07:45<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using the BERT_geoparser Data.py module\n",
    "data_csv = r'../data/ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "\n",
    "model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "results = model.results_dataframe(texts=review_df.text.values, include_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jshin\\Documents\\Work\\DSO\\BERT_geoparser\\BERT_geoparser\\retagger.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  groups.append(current_group)\n",
      "100%|██████████| 5800/5800 [48:59<00:00,  1.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "retagger = Retagger(results)\n",
    "review_df = review_df.reset_index()\n",
    "retagger.retag(['geo', 'gpe', 'org'], threshold='bbox', review_df=review_df)\n",
    "test_data = retagger.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('data/step_4/test_yelp_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 25s 135ms/step\n"
     ]
    }
   ],
   "source": [
    "data_csv = 'data/step_4/test_yelp_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "tar_model = BertModel(saved_model='20230926_tar_tagged_bert_model_large.hdf5', data=data)\n",
    "X_tokens, y_pred, y_true = tar_model.test('data/step_4/test_yelp_dataset.csv', return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"O\" accuracy : 0.994\n",
      "\"O\" precision : 0.991\n",
      "\"O\" recall : 0.994\n",
      "=======================\n",
      "\"tar\" accuracy : 0.799\n",
      "\"tar\" precision : 0.822\n",
      "\"tar\" recall : 0.802\n",
      "=======================\n",
      "\"inc\" accuracy : 0.679\n",
      "\"inc\" precision : 0.773\n",
      "\"inc\" recall : 0.691\n",
      "=======================\n",
      "macro average recall : 0.733\n",
      "macro average precision : 0.826\n",
      "micro average recall : 0.983\n",
      "micro average precision : 0.983\n"
     ]
    }
   ],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'tar', 'inc']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 3)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),3)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = {'Sentence #': [], 'Word':[], 'Tag':[], 'Predicted Tag':[]}\n",
    "for i, (X, y, y_hat) in enumerate(zip(X_tokens, y_true, y_pred)):\n",
    "    rows['Sentence #'].extend([i]*len(X))\n",
    "    rows['Word'].extend(X)\n",
    "    rows['Tag'].extend(y)\n",
    "    rows['Predicted Tag'].extend(y_hat)\n",
    "results_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('data/predictions/20230927_tar_inc_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model improvement\n",
    "While these results are promising, there is a serious flaw in the datset the model has been trained on. Specifically, the reviews are from businesses from a small number of locations. As such, there is a danger that the model is simply consistently assigning the `tar` tag to tokens matching those places. To investigate we will build a new dataset with all the tokens tagged as `tar` replaced with random draws from a list of place names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jshin\\Documents\\Work\\DSO\\BERT_geoparser\\BERT_geoparser\\retagger.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df.loc[:, 'sequential_group'] = groups\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>B-art</th>\n",
       "      <th>B-eve</th>\n",
       "      <th>B-geo</th>\n",
       "      <th>B-gpe</th>\n",
       "      <th>B-nat</th>\n",
       "      <th>B-org</th>\n",
       "      <th>B-per</th>\n",
       "      <th>...</th>\n",
       "      <th>I-geo</th>\n",
       "      <th>I-gpe</th>\n",
       "      <th>I-nat</th>\n",
       "      <th>I-org</th>\n",
       "      <th>I-per</th>\n",
       "      <th>I-tim</th>\n",
       "      <th>O</th>\n",
       "      <th>old_tag</th>\n",
       "      <th>Tag</th>\n",
       "      <th>sequential_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>phil</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>B-tar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>##ly</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>B-tar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>897</td>\n",
       "      <td>9</td>\n",
       "      <td>shell</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.041</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>B-tar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>898</td>\n",
       "      <td>9</td>\n",
       "      <td>key</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.027</td>\n",
       "      <td>I-geo</td>\n",
       "      <td>I-tar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>899</td>\n",
       "      <td>9</td>\n",
       "      <td>island</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.084</td>\n",
       "      <td>I-geo</td>\n",
       "      <td>I-tar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Sentence #    Word  B-art  B-eve  B-geo  B-gpe  B-nat  B-org  \\\n",
       "66      66           0    phil  0.000  0.000  0.981  0.001  0.000  0.007   \n",
       "67      67           0    ##ly  0.000  0.000  0.987  0.001  0.000  0.006   \n",
       "897    897           9   shell  0.037  0.003  0.469  0.002  0.001  0.358   \n",
       "898    898           9     key  0.004  0.002  0.061  0.001  0.001  0.004   \n",
       "899    899           9  island  0.002  0.000  0.011  0.001  0.000  0.002   \n",
       "\n",
       "     B-per  ...  I-geo  I-gpe  I-nat  I-org  I-per  I-tim      O  old_tag  \\\n",
       "66   0.000  ...  0.001  0.000  0.000  0.000  0.000  0.000  0.005    B-geo   \n",
       "67   0.000  ...  0.002  0.000  0.000  0.000  0.000  0.000  0.002    B-geo   \n",
       "897  0.047  ...  0.011  0.000  0.001  0.015  0.002  0.002  0.041    B-geo   \n",
       "898  0.009  ...  0.583  0.003  0.001  0.212  0.048  0.004  0.027    I-geo   \n",
       "899  0.002  ...  0.619  0.002  0.001  0.218  0.023  0.003  0.084    I-geo   \n",
       "\n",
       "       Tag  sequential_group  \n",
       "66   B-tar                 0  \n",
       "67   B-tar                 0  \n",
       "897  B-tar                 1  \n",
       "898  I-tar                 1  \n",
       "899  I-tar                 1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the test data\n",
    "test_data = pd.read_csv('data/step_4/test_yelp_dataset.csv')\n",
    "# extract only lines tagged as target\n",
    "target_only_data = test_data[test_data.Tag.str.contains('tar')]\n",
    "# use the Retagger class to add a 'sequential group' column to this data.\n",
    "retagger = Retagger(target_only_data)\n",
    "retagger.add_sequential_groups()\n",
    "retagger.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from BERT_geoparser.utils import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_cities_df = pd.read_csv('data/model_improvement/worldcities.csv')\n",
    "us_cities_df = world_cities_df[world_cities_df.iso3=='USA']\n",
    "us_cities = []\n",
    "for i, city in us_cities_df.iterrows():\n",
    "    name = city.city_ascii\n",
    "    lat = city.lat\n",
    "    lng = city.lng\n",
    "    us_cities.append((name, (lat,lng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {}\n",
    "\n",
    "for i, group in retagger.df.groupby('sequential_group'):\n",
    "    phrase = Phrase('', tag=None)\n",
    "    for token, tag in zip(group['Word'].values, group['Tag'].values):\n",
    "        phrase.add_token(token=token, tag=tag)\n",
    "    if phrase.text not in replacement_dict.keys():\n",
    "        new_cities = random.choices(us_cities, k=2)\n",
    "        # make sure we're not replacing with the same city\n",
    "        new_city = new_cities[0]\n",
    "        if new_city == phrase.text:\n",
    "            new_city == new_cities[1]\n",
    "        replacement_dict.update({phrase.text:new_city})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the review dataset and the accompanying business info dataset\n",
    "review_df = pd.read_csv('data/step_2/yelp_reviews_with_location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \n",
      "\n",
      "The food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\n",
      "(40.2101961875, -75.2236385919)\n"
     ]
    }
   ],
   "source": [
    "print(review_df.iloc[0].text)\n",
    "print(review_df.iloc[0].coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each review and replace any identified target locations using the dictionart\n",
    "for i, review in review_df.iterrows():\n",
    "    text = review.text.lower()\n",
    "    words = text.split(' ')\n",
    "    new_coordinates = review.coordinates\n",
    "    for w in words:\n",
    "        if w in replacement_dict.keys():\n",
    "            new_text = text.replace(w, replacement_dict[w][0])\n",
    "            new_coordinates = replacement_dict[w][1]\n",
    "            review_df.loc[i, 'text'] = new_text\n",
    "            review_df.loc[i, 'coordinates'] = str(new_coordinates)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. we have tried it multiple times, because i want to like it! i have been to it's other locations in Peters and never had a bad experience. \n",
      "\n",
      "the food is good, but it takes a very long time to come out. the waitstaff is very young, but usually pleasant. we have just had too many experiences where we spent way too long waiting. we usually opt for another diner or restaurant on the weekends, in order to be done quicker.\n",
      "(40.2739, -80.0803)\n"
     ]
    }
   ],
   "source": [
    "# Check the first entry: NJ should have been replaced with a new place and the location updated\n",
    "print(review_df.iloc[0].text)\n",
    "print(review_df.iloc[0].coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.to_csv('data/model_improvement/randomised_review_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 6s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "data_csv = 'data/step_4/test_data_random_locations.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n",
    "tar_model = BertModel(saved_model='20230926_tar_tagged_bert_model_large.hdf5', data=data)\n",
    "X_tokens, y_pred, y_true = tar_model.test('data/step_4/test_data_random_locations.csv', return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"O\" accuracy : 0.996\n",
      "\"O\" precision : 0.996\n",
      "\"O\" recall : 0.996\n",
      "=======================\n",
      "\"tar\" accuracy : 0.609\n",
      "\"tar\" precision : 0.876\n",
      "\"tar\" recall : 0.613\n",
      "=======================\n",
      "\"inc\" accuracy : 0.858\n",
      "\"inc\" precision : 0.847\n",
      "\"inc\" recall : 0.874\n",
      "=======================\n",
      "macro average recall : 0.753\n",
      "macro average precision : 0.886\n",
      "micro average recall : 0.989\n",
      "micro average precision : 0.989\n"
     ]
    }
   ],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'tar', 'inc']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 3)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),3)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>B-inc</th>\n",
       "      <th>B-tar</th>\n",
       "      <th>I-inc</th>\n",
       "      <th>I-tar</th>\n",
       "      <th>O</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>donnie</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>B-inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>##s</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.617</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.392</td>\n",
       "      <td>I-inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>heaven</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.238</td>\n",
       "      <td>I-inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>in</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>derby</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>B-tar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>do</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>best</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>deep</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>pan</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.955</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>pizza</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.996</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>outside</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>of</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>lock</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>B-inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>##port</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.009</td>\n",
       "      <td>B-inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #     Word  B-inc  B-tar  I-inc  I-tar      O    Tag\n",
       "0            0    [CLS]  0.001  0.000  0.000  0.000  0.999      O\n",
       "1            0   donnie  0.692  0.010  0.002  0.000  0.295  B-inc\n",
       "2            0      ##s  0.126  0.004  0.242  0.010  0.617      O\n",
       "3            0    pizza  0.008  0.000  0.582  0.017  0.392  I-inc\n",
       "4            0   heaven  0.004  0.000  0.731  0.025  0.238  I-inc\n",
       "5            0       in  0.000  0.000  0.004  0.000  0.995      O\n",
       "6            0    derby  0.405  0.591  0.001  0.001  0.002  B-tar\n",
       "7            0       do  0.002  0.000  0.001  0.000  0.996      O\n",
       "8            0      the  0.000  0.000  0.000  0.000  1.000      O\n",
       "9            0     best  0.000  0.000  0.000  0.000  1.000      O\n",
       "10           0     deep  0.003  0.000  0.001  0.000  0.996      O\n",
       "11           0      pan  0.001  0.000  0.044  0.000  0.955      O\n",
       "12           0    pizza  0.000  0.000  0.004  0.000  0.996      O\n",
       "13           0  outside  0.000  0.000  0.000  0.000  1.000      O\n",
       "14           0       of  0.000  0.000  0.000  0.000  1.000      O\n",
       "15           0     lock  0.610  0.389  0.000  0.000  0.001  B-inc\n",
       "16           0   ##port  0.534  0.306  0.113  0.037  0.009  B-inc\n",
       "17           0    [SEP]  0.000  0.000  0.000  0.000  1.000      O"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_model.results_dataframe(texts=[\"Donnies Pizza Heaven in Derby do the best deep pan pizza outside of Lockport\"], include_best=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'philly': ('Oxford', (42.1286, -71.8665)),\n",
       " 'shell key island': ('Vestal', (42.0492, -76.026)),\n",
       " 'chinatown': ('Boulder City', (35.8407, -114.9257)),\n",
       " 'newtown': ('Sharonville', (39.2825, -84.4071)),\n",
       " 'tennessee': ('Mableton', (33.8133, -84.5655)),\n",
       " 'center city': ('Holly Springs', (35.6526, -78.8399)),\n",
       " 'conicelli honda': ('Orangetown', (41.0527, -73.9475)),\n",
       " 'nashville': ('Naugatuck', (41.489, -73.0518)),\n",
       " 'tampa': ('Richmond Hill', (31.9012, -81.3125)),\n",
       " 'philadelphia': ('West Springfield', (38.7771, -77.2268)),\n",
       " 'ybor city': ('Hendersonville', (35.3247, -82.4575)),\n",
       " 'california': ('Kennedy', (40.4768, -80.1028)),\n",
       " 'rittenhouse square': ('Pleasantville', (39.39, -74.5169)),\n",
       " 'barnes jewish': ('Garden City', (43.6526, -116.2743)),\n",
       " 'sonora': ('Warminster', (40.2043, -75.0915)),\n",
       " 'arizona': ('Spanish Fork', (40.1101, -111.6405)),\n",
       " 'az': ('Dunkirk', (42.4801, -79.3324)),\n",
       " ' mi': ('East Ridge', (34.9973, -85.2285)),\n",
       " 'reno': ('Howell', (42.6078, -83.9339)),\n",
       " 'boise': ('Lockport', (41.5906, -88.0293)),\n",
       " 'nola': ('Vincent', (34.0983, -117.9238)),\n",
       " 'new orleans': ('Alma', (43.3799, -84.6556)),\n",
       " 'honeygrow': ('Solebury', (40.3676, -75.0032)),\n",
       " 'treasure island': ('Henderson', (36.3256, -78.4151)),\n",
       " 'tucson': ('Marlton', (39.9014, -74.9294)),\n",
       " 'brandon': ('Naples', (26.1504, -81.7936)),\n",
       " 'mid city': ('Clifton', (40.863, -74.1575)),\n",
       " 'brentwood': ('Suitland', (38.8492, -76.9225)),\n",
       " 'french quarter': ('Alum Rock', (37.3694, -121.8238)),\n",
       " 'noboru': ('Galt', (38.2698, -121.3004)),\n",
       " 'indiana': ('Martin', (36.3385, -88.8513)),\n",
       " 'mo': ('Fairview Park', (41.4419, -81.853)),\n",
       " 'mississippi river': ('East Patchogue', (40.7704, -72.9817)),\n",
       " 'florida': ('South Plainfield', (40.5748, -74.4152)),\n",
       " 'cafe fleur de lis': ('Kirkland', (47.697, -122.2057)),\n",
       " 'st. charles': ('Springdale', (39.2909, -84.476)),\n",
       " 'oldsmar': ('Mineral Wells', (32.8169, -98.0776)),\n",
       " 'penllyn': ('Angola', (41.6433, -85.005)),\n",
       " 'edmonton': ('Cheshire', (41.5113, -72.9036)),\n",
       " 'santa barbara': ('Hope Mills', (34.971, -78.9597)),\n",
       " 'university city': ('Absecon', (39.4229, -74.4944)),\n",
       " 'manayunk': ('Appleton', (44.278, -88.3892)),\n",
       " 'drexel hill': ('Presque Isle', (46.6868, -67.9874)),\n",
       " 'upper darby': ('Chester', (39.8456, -75.3718)),\n",
       " 'pennsylvania': (\"O'Fallon\", (38.5974, -89.9148)),\n",
       " 'kenner': ('Mechanicsburg', (40.2115, -77.006)),\n",
       " 'idaho': ('Wilson', (35.7311, -77.9284)),\n",
       " 'louisiana': ('Elizabethtown', (37.7031, -85.8773)),\n",
       " 'hotel mazarin': ('Salem', (44.9233, -123.0244)),\n",
       " 'stearns wharf': ('Ridgefield', (40.8313, -74.0147)),\n",
       " 'clearwater beach': ('Magna', (40.7634, -112.1599)),\n",
       " 'royal sonesta new orleans': ('Buckeye', (33.4314, -112.6429)),\n",
       " 'citrus park': ('Oildale', (35.4293, -119.0306)),\n",
       " 'williamson': ('Middleton', (42.6043, -71.0164)),\n",
       " 'glenside': ('Booneville', (34.6643, -88.5684)),\n",
       " 'benton park': ('Bothell', (47.7735, -122.2044)),\n",
       " 'indianapolis': ('Beaumont', (30.0849, -94.1451)),\n",
       " 'treme': ('Highland City', (27.9633, -81.8781)),\n",
       " 'id': ('Derby', (37.5571, -97.2551)),\n",
       " 'clearwater': ('Odenton', (39.0661, -76.6938)),\n",
       " 'charro steak': ('Fallston', (39.5332, -76.4452)),\n",
       " 'olin library': ('Mountain Brook', (33.4871, -86.74)),\n",
       " 'missouri': ('Parkland', (26.3219, -80.2533)),\n",
       " 'westmont': ('Merrydale', (30.4998, -91.1081)),\n",
       " 'st. louis': ('Hampton', (37.0551, -76.363)),\n",
       " 'reading terminal market': ('Kensington', (41.6284, -72.7686)),\n",
       " 'reading terminal': ('Chelsea', (33.3262, -86.63)),\n",
       " 'ardmore': ('South Fulton', (33.6273, -84.58)),\n",
       " 'tn.': ('Twinsburg', (41.322, -81.4451)),\n",
       " 'metairie': ('Laredo', (27.5625, -99.4874)),\n",
       " 'preservation hall': ('McAlester', (34.9257, -95.7734)),\n",
       " 'kirkwood': ('St. Francis', (42.9716, -87.873)),\n",
       " 'hipcityveg': ('Pelham', (33.3114, -86.7573)),\n",
       " 'chestnut hill': ('Westchester', (41.8492, -87.8906)),\n",
       " 'east nashville': ('Oakbrook', (38.9996, -84.6797)),\n",
       " 'carmel': ('Kaysville', (41.029, -111.9456)),\n",
       " 'abs': ('New London', (41.3502, -72.1022)),\n",
       " 'tn': ('San Gabriel', (34.0949, -118.099)),\n",
       " 'union station': ('Moorestown', (39.9784, -74.9413)),\n",
       " 'pinellas park': ('Nesconset', (40.8467, -73.1522)),\n",
       " 'londonderry': ('Fairfield', (41.0064, -91.9667)),\n",
       " 'ventura': ('Locust Grove', (33.3446, -84.1071)),\n",
       " 'soulard': ('Maple Shade', (39.952, -74.995)),\n",
       " 'bourbon street': ('Westland', (42.3192, -83.3805)),\n",
       " 'gulfport': ('Pleasant Hill', (37.9539, -122.0759)),\n",
       " 'spring hill': ('Green Valley', (39.3414, -77.24)),\n",
       " 'rittenhouse': ('South Riding', (38.912, -77.5132)),\n",
       " 'france': ('Wakefield', (42.5035, -71.0656)),\n",
       " 'johns pass': ('Keene', (42.9494, -72.2998)),\n",
       " 'nj': ('Orem', (40.2981, -111.6994)),\n",
       " 'south philly': ('Grafton', (43.3204, -87.948)),\n",
       " 'hillsboro village': ('Tredyffrin', (40.0663, -75.454)),\n",
       " 'broad ripple': ('Lakeside', (30.1356, -81.7674)),\n",
       " 'gaylord opryland resort': ('Franklin Park', (40.5903, -80.0999)),\n",
       " 'clayton': ('Orangeburg', (33.4928, -80.8671)),\n",
       " 'village whiskey': ('Saraland', (30.8479, -88.1004)),\n",
       " 'ybor': ('Carson', (33.8374, -118.2559)),\n",
       " 'dda': ('Pell City', (33.561, -86.2669)),\n",
       " 'centennial hall': ('Whittier', (33.9678, -118.0188)),\n",
       " 'bourbon st': ('Shelby', (35.2904, -81.5451)),\n",
       " 'irvington': ('Bethlehem', (42.5856, -73.8219)),\n",
       " 'pitman': ('Hobbs', (32.7281, -103.16)),\n",
       " 'fairmount': ('London Grove', (39.8327, -75.8155)),\n",
       " '30th street': ('Frederick', (40.1088, -104.9701)),\n",
       " 'foodery': ('Washington', (40.1741, -80.2465)),\n",
       " 'prussia': ('Lantana', (26.5834, -80.0564)),\n",
       " 'bridgestone arena': ('Cornelius', (35.4724, -80.8813)),\n",
       " 'holy cross': ('Alton', (38.9037, -90.152)),\n",
       " 'conicelli': ('Auburn Hills', (42.6735, -83.2448)),\n",
       " 'il': ('Eureka', (38.5004, -90.6491)),\n",
       " 'loews philadelphia': ('Coatesville', (39.9849, -75.82)),\n",
       " 'grand sierra resort': ('Portales', (34.1754, -103.3565)),\n",
       " 'kimberton whole foods': ('New Lenox', (41.5095, -87.9703)),\n",
       " 'hickory hollow mall': (\"D'Iberville\", (30.4709, -88.9011)),\n",
       " 'antioch': ('Haines City', (28.1102, -81.6157)),\n",
       " 'greenbrier': ('Boonton', (40.9047, -74.4048)),\n",
       " 'west edmonton mall': ('Hopewell', (37.2915, -77.2985)),\n",
       " 'oro valley': ('Indianola', (41.3629, -93.5652)),\n",
       " 's philly': ('Channelview', (29.7914, -95.1144)),\n",
       " 'bella vista': ('Burlington', (48.4676, -122.3298)),\n",
       " 'tower grove': ('Addison', (32.959, -96.8355)),\n",
       " 'speedway': ('Bernards', (40.6761, -74.5677)),\n",
       " 'n.o.': ('Parker', (39.5084, -104.7753)),\n",
       " 'mood cafe': ('Fairbanks', (64.8353, -147.6533)),\n",
       " 'grand hyatt tampa': ('Mill Valley', (37.9086, -122.5421)),\n",
       " 'arnold': ('Haverford', (39.9868, -75.3164)),\n",
       " 'st louis': ('Archdale', (35.9032, -79.9591)),\n",
       " 'saint louis': ('Hope', (33.6682, -93.5895)),\n",
       " 'sbraga': ('Johnstown', (40.3499, -104.9482)),\n",
       " 'pa': ('Barberton', (41.0095, -81.6037)),\n",
       " 'delaware': ('Pearl', (32.273, -90.0918)),\n",
       " 'pancake pantry': ('Merrimack', (42.8547, -71.5188)),\n",
       " 'sand key': ('Snellville', (33.8562, -84.0038)),\n",
       " 'westchase': ('Alhambra', (34.084, -118.1355)),\n",
       " 'loews': ('Marrero', (29.8871, -90.1126)),\n",
       " 'us': ('Miramar Beach', (30.3854, -86.3442)),\n",
       " 'society hill': ('Geneseo', (42.8038, -77.7783)),\n",
       " 'wesley chapel': ('Gadsden', (34.0086, -86.0157)),\n",
       " 'narberth': ('Newark', (37.5201, -122.0307)),\n",
       " 'jenkintown': ('Gardena', (33.8943, -118.3072)),\n",
       " 'zeppoli': ('Shepherdsville', (37.9813, -85.7007)),\n",
       " 'collingswood': ('Fort Meade', (39.1061, -76.7437)),\n",
       " 'mt. airy': ('Wekiwa Springs', (28.6984, -81.4251)),\n",
       " 'gypsy saloon': ('Oak Forest', (41.6054, -87.7527)),\n",
       " 'longwood gardens': ('New York', (40.6943, -73.9249)),\n",
       " 'pinellas': ('Lowell', (42.6389, -71.3217)),\n",
       " 'florissant': ('Owosso', (42.9955, -84.176)),\n",
       " 'haddonfield': ('Minot', (48.2375, -101.278)),\n",
       " \"o'fallon\": ('Elmhurst', (41.8973, -87.9432)),\n",
       " 'skippack': ('La Plata', (38.5352, -76.97)),\n",
       " 'st. petersburg': ('Ruskin', (27.7065, -82.4209)),\n",
       " 'pennhurst': ('St. Peter', (44.3295, -93.9658)),\n",
       " 'pizzeria vetri': ('Augusta', (44.3341, -69.7319)),\n",
       " 'fishers': ('Galesburg', (40.9506, -90.3763)),\n",
       " 'mi': ('Tavares', (28.792, -81.7352)),\n",
       " 'flourtown': ('Cameron Park', (38.6738, -120.9872)),\n",
       " 'maple shade': ('Kirksville', (40.1986, -92.5752)),\n",
       " 'broad ripple brewpub': ('Beckett Ridge', (39.3448, -84.438)),\n",
       " 'bryn mawr': ('Irondale', (33.5439, -86.6606)),\n",
       " 'peppermill reno': ('Chesterfield', (40.1166, -74.6459)),\n",
       " 'nevada': ('Avon', (39.7601, -86.3916)),\n",
       " 'west chester': ('Lewiston', (43.1793, -78.971)),\n",
       " 'brewerytown': ('Marshfield', (42.114, -70.7151)),\n",
       " 'zionsville': ('Pine', (40.6437, -80.0315)),\n",
       " 'exton': ('Plant City', (28.014, -82.1201)),\n",
       " 'saint autograph hotel': ('Southaven', (34.9514, -89.9786)),\n",
       " 'philly chinatown': ('Le Ray', (44.0771, -75.7975)),\n",
       " 'new jersey': ('Dayton', (39.2592, -119.5653)),\n",
       " 'fishtown': ('Shreveport', (32.4653, -93.7955)),\n",
       " 'uptown': ('Lakeway', (30.3544, -97.9864)),\n",
       " 'voorhees': ('North Fair Oaks', (37.4754, -122.2034)),\n",
       " 'pinellas county': ('Front Royal', (38.926, -78.1838)),\n",
       " 'de': ('Warrington', (30.3835, -87.2946)),\n",
       " \"pal ' s lounge\": ('Des Moines', (47.3914, -122.3156)),\n",
       " 'mid-city': ('Lansing', (42.5667, -76.5316)),\n",
       " 'ais': ('San Gabriel', (34.0949, -118.099)),\n",
       " 'dk donuts': ('Brighton', (39.9716, -104.7963)),\n",
       " \"vieux carre '\": ('Summerville', (33.0016, -80.1799)),\n",
       " 'riverview': ('Lomita', (33.7933, -118.3175)),\n",
       " 'cafe du monde': ('Bethany', (45.5614, -122.837)),\n",
       " 'bucks county': ('Yorkshire', (38.7882, -77.4496)),\n",
       " 'st.louis': ('Wade Hampton', (34.8821, -82.3336)),\n",
       " 'glanzmann': ('Titusville', (28.5727, -80.8193)),\n",
       " 'st. pete beach': ('Gulf Hills', (30.4367, -88.815)),\n",
       " ' norristown': ('Long Beach', (40.5887, -73.666)),\n",
       " 'mt. juliet': ('Moorhead', (46.8673, -96.7461)),\n",
       " 'montecito': ('Manchester', (35.463, -86.0774)),\n",
       " 'tarpon springs': ('McCalla', (33.3023, -87.0302)),\n",
       " 'mt laurel': ('Grand Island', (40.9218, -98.3586)),\n",
       " 'siam cafe': ('Valley Cottage', (41.1162, -73.9433)),\n",
       " 'city center park': ('Rochester', (44.0154, -92.478)),\n",
       " 'indiana convention center': ('Lafayette', (39.9946, -105.0998)),\n",
       " 'ssa': ('Bridgeton', (38.7673, -90.4275)),\n",
       " 'plant city': ('Warwick', (40.2503, -75.0818)),\n",
       " \"caruso ' s\": ('Sylvania', (41.71, -83.7092)),\n",
       " 'palm harbor': ('Hermantown', (46.8057, -92.2407)),\n",
       " 'garden district': ('Pitman', (39.7335, -75.1306)),\n",
       " ' son': ('East Islip', (40.7257, -73.1869)),\n",
       " 'doylestown': ('Trenton', (39.4792, -84.462)),\n",
       " 'royal street': ('Tomball', (30.0951, -95.6194)),\n",
       " 'jersey java & tea': ('St. Paul', (44.9478, -93.1039)),\n",
       " 'paradice shave': ('Booneville', (34.6643, -88.5684)),\n",
       " 'usa': ('Arnold', (39.0437, -76.4974)),\n",
       " 'castleton': ('Middleburg Heights', (41.3696, -81.815))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
