{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# thrid party imports\n",
    "import sys\n",
    "sys.path.append('../BERT_geoparser/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "# local imports\n",
    "from train_model import Trainer\n",
    "from tokenizer import Tokenizer\n",
    "from data import Data\n",
    "from model import BertModel\n",
    "from analysis import Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-tuning a BERT language model on NER data\n",
    "In this notebook we use the `BERT_geoparser` package to build and fine tune a BERT model to perform Named Entity Recognition (NER) tasks. This is the first step in a multi-step process to build and train a BERT model to identify target and incidental locations within text. \n",
    "\n",
    "We use an NER dataset labelled using the B-I-O format, with 8 categories of word - location (`geo`), time (`tim`), organization (`org`), person (`per`), geo-political entity (`gpe`), art/culture (`art`), event (`eve`) or nature (`nat`). Each tag can indicate whether a word is the *begining* of a related phrase (`B`) or *inside* a phrase (`I`). Words which do not belong to any category are given the *outer* tag (`O`). Specialtokens indicating the start (`CLS`) and end (`SEP`) of a sentence are also added. For example, the phrase:\n",
    "\n",
    "<p style=\"text-align: center;\"><span style=\"color:red\">Jane</span> visited <span style=\"color:green\">Madisson Square Gardens</span> while in <span style=\"color:yellow\">New York</span>.</p>\n",
    "\n",
    "Would receive the tags:\n",
    "\n",
    "<p style=\"text-align: center;\"> [CLS] <span style=\"color:red\"> [B-PER] </span> [O] <span style=\"color:green\">[B-ORG] [I-ORG] [I-ORG]</span> [O] [O] <span style=\"color:yellow\">[B-GEO] [I-GEO]</span> [SEP] </p>\n",
    "\n",
    "The Fine tuned bert model can then estimate the most likely sequence of tags for a given sentence, and can provide the confidence on the given tags.\n",
    "\n",
    "## 1.1 Initial training on the CoNLL dataset\n",
    "We will initially train the model on CoNLL-2003 dataset, before retraining the same model on the WikiNeural dataset. This will provide the model with a large number of trainingexamples, while ensuring it is optimized to handel wikipedia style data. This can also be done by running the script `train_model_conll2003.sh` in a Linux Torque environment.\n",
    "\n",
    "Running this cell will train and test the model. The precision, recall and F1 score in each category are given in the `CoNLL_test_results.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data_path = r'../data/NB1/train_CoNLL_dataset.csv', \n",
    "                  model_size = 'large',\n",
    "                  cased = True, \n",
    "                  learning_rate=2e-6,\n",
    "                  max_len=80,\n",
    "                  saved_model=False)\n",
    "\n",
    "trainer.train(save_as='r../models/TopoBERT_CoNLL.hdf5',\n",
    "              n_epochs=20,\n",
    "              batch_size=4,\n",
    "              val_split=0.1) \n",
    "              \n",
    "trainer.test(test_data=r'../data/NB1/test_CoNLL_dataset.csv',\n",
    "             results_filename=r'../results/NB1/CoNLL_test_results.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Retraining on the WikiNeural dataset\n",
    "We can now retrain the saved model on Wikipedia data, using the WikiNeural dataset. Once again, this can be run using `train_model_wikineural.sh`. Test results are given in `wikineural_test_results.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_path = 'wikineural_train_dataset.csv', \n",
    "                  model_size = 'Large',\n",
    "                  cased = True, \n",
    "                  learning_rate=2e-6,\n",
    "                  max_len=80,\n",
    "                  saved_model=r'../models/TopoBERT.hdf5')\n",
    "\n",
    "trainer.train(save_as='../models/TopoBERT_WikiNeural.hdf5',\n",
    "              n_epochs=5,\n",
    "              batch_size=2,\n",
    "              val_split=0.1) \n",
    "              \n",
    "trainer.test(test_data=r'../data/NB1/wikineural_test_dataset.csv',\n",
    "             results_filename=r'../results/NB1/wikineural_test_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
