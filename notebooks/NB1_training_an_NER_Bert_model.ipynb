{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BERT_geoparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jshin\\Documents\\Work\\DSO\\multi_lm\\notebooks\\NB1_training_an_NER_Bert_model.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jshin/Documents/Work/DSO/multi_lm/notebooks/NB1_training_an_NER_Bert_model.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# local imports\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jshin/Documents/Work/DSO/multi_lm/notebooks/NB1_training_an_NER_Bert_model.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jshin/Documents/Work/DSO/multi_lm/notebooks/NB1_training_an_NER_Bert_model.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jshin/Documents/Work/DSO/multi_lm/notebooks/NB1_training_an_NER_Bert_model.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m BertModel\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jshin/Documents/Work/DSO/multi_lm/notebooks/NB1_training_an_NER_Bert_model.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39manalysis\u001b[39;00m \u001b[39mimport\u001b[39;00m Results\n",
      "File \u001b[1;32mc:\\Users\\Jshin\\Documents\\Work\\DSO\\multi_lm\\notebooks\\../multi_lm\\data.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing\n\u001b[0;32m     12\u001b[0m \u001b[39m# local imports\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mBERT_geoparser\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mBERT_geoparser\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m flatten\n\u001b[0;32m     16\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mData\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'BERT_geoparser'"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "import sys\n",
    "sys.path.append('../BERT_geoparser/')\n",
    "# thrid party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# local imports\n",
    "from tokenizer import Tokenizer\n",
    "from data import Data\n",
    "from model import BertModel\n",
    "from analysis import Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT language model on NER data\n",
    "In this notebook we use the `BERT_geoparser` package to build and fine tune a BERT model to perform Named Entity Recognition (NER) tasks. This is the first step in a multi-step process to build and train a BERT model to identify target and incidental locations within text. \n",
    "\n",
    "We use an NER dataset labelled using the B-I-O format, with 8 categories of word - location (`geo`), time (`tim`), organization (`org`), person (`per`), geo-political entity (`gpe`), art/culture (`art`), event (`eve`) or nature (`nat`). Each tag can indicate whether a word is the *begining* of a related phrase (`B`) or *inside* a phrase (`I`). Words which do not belong to any category are given the *outer* tag (`O`). Specialtokens indicating the start (`CLS`) and end (`SEP`) of a sentence are also added. For example, the phrase:\n",
    "\n",
    "<p style=\"text-align: center;\"><span style=\"color:red\">Jane</span> visited <span style=\"color:green\">Madisson Square Gardens</span> while in <span style=\"color:yellow\">New York</span>.</p>\n",
    "\n",
    "Would receive the tags:\n",
    "\n",
    "<p style=\"text-align: center;\"> [CLS] <span style=\"color:red\"> [B-PER] </span> [O] <span style=\"color:green\">[B-ORG] [I-ORG] [I-ORG]</span> [O] [O] <span style=\"color:yellow\">[B-GEO] [I-GEO]</span> [SEP] </p>\n",
    "\n",
    "The Fine tuned bert model can then estimate the most likely sequence of tags for a given sentence, and can provide the confidence on the given tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset using the BERT_geoparser Data.py module\n",
    "data_csv = r'data/step_1/train_ner_dataset.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 125,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 125, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 125, 18)      13842       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,496,082\n",
      "Trainable params: 109,496,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jshin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new BERTModel object\n",
    "model = BertModel(saved_model=None, data=data)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2159/2159 [==============================] - 541s 243ms/step - loss: 0.0381 - accuracy: 0.2015 - val_loss: 0.0289 - val_accuracy: 0.2036\n",
      "Epoch 2/2\n",
      "2159/2159 [==============================] - 574s 266ms/step - loss: 0.0239 - accuracy: 0.2049 - val_loss: 0.0283 - val_accuracy: 0.2040\n"
     ]
    }
   ],
   "source": [
    "model.train(save_as='20230808_bert_model_large.hdf5', n_epochs=2, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on new data\n",
    "We wil consider the recall and precision across each category, where, for each category, $C$, categorical recall, $r_C$, and precision, $p_C$ are defined in terms of the number of true positives, $TP_C$, false positives, $FP_C$ and false negatives $FN_C$ in each category, such that:\n",
    "\n",
    "$$ r_C = \\frac{TP_C}{FN_C + TP_C}, $$\n",
    "$$ p_C = \\frac{TP_C}{FP_C + TP_C}. $$\n",
    "\n",
    "We will also consider the micro averaged recall, $\\mu_r$, and precision $\\mu_p$; and macro averaged recall, $\\nu_r$, and precision $\\nu_p$. For a model with categories $C \\in \\{ 1,2,...,N \\}$, this is given as: \n",
    "\n",
    "$$ \\nu_r = \\frac{\\sum_{C=1}^{N}r_C}{N},$$\n",
    "$$ \\nu_r = \\frac{\\sum_{C=1}^{N}r_C}{N},$$\n",
    "\n",
    "$$ \\mu_r = \\frac{\\sum_{C=1}^{N}TP_C}{\\sum_{C=1}^{N}(TP_C + FN_C)},$$\n",
    "$$ \\mu_p = \\frac{\\sum_{C=1}^{N}TP_C}{\\sum_{C=1}^{N}(TP_C + FP_C)}.$$\n",
    "\n",
    "Considering both micro and macro averaged statistics lets us better understand how class imbalances interact with our model results. The macro averaged statistics treat all classes equally, regardless of number of occurances. The micro averaged statistic gives an equal weight to each sample in the dataset, which can be helpful when there is a class imbalance. In this dataset the 'O' class is significantly larger than any other class, so the micro average is likely to be more important.\n",
    "\n",
    "For mathematical reason which aren't too important here, $\\mu_r$ and $\\mu_p$ will always give the same value, as will a micro averaged F1 score. Hence, we will also consider a macro-averaged F1 statistic given by:\n",
    "\n",
    "$$ F_{\\nu} = 2 \\times \\frac{\\nu_p \\cdot \\nu_r}{\\nu_p + \\nu_r}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 42s 129ms/step\n"
     ]
    }
   ],
   "source": [
    "#model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "y_pred, y_true = model.test('data/step_1/test_ner_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"O\" accuracy : 0.991\n",
      "\"O\" precision : 0.988\n",
      "\"O\" recall : 0.991\n",
      "=======================\n",
      "\"geo\" accuracy : 0.89\n",
      "\"geo\" precision : 0.835\n",
      "\"geo\" recall : 0.901\n",
      "=======================\n",
      "\"per\" accuracy : 0.913\n",
      "\"per\" precision : 0.85\n",
      "\"per\" recall : 0.944\n",
      "=======================\n",
      "\"gpe\" accuracy : 0.942\n",
      "\"gpe\" precision : 0.972\n",
      "\"gpe\" recall : 0.943\n",
      "=======================\n",
      "\"org\" accuracy : 0.642\n",
      "\"org\" precision : 0.82\n",
      "\"org\" recall : 0.66\n",
      "=======================\n",
      "macro average recall : 0.592312\n",
      "macro average precision : 0.783257\n",
      "micro average recall : 0.962\n",
      "micro average precision : 0.962\n",
      "macro average F1 : 0.675\n"
     ]
    }
   ],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'geo', 'per', 'gpe', 'org']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 6)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),6)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')\n",
    "print(f'macro average F1 : {np.round(res.macro_average_F1(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall on the 'geo' category is pretty poor. We'll try and reduce the number of categories the model is guessing and see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove all tags that arent in the new tagging system\n",
    "# test data\n",
    "test_data = pd.read_csv('data/step_1/test_ner_dataset.csv')\n",
    "new_tags = ['B-geo', 'I-geo', 'B-gpe', 'I-gpe', 'B-org', 'I-org', 'B-per', 'I-per']\n",
    "test_data['Tag'] = [x if x in new_tags else 'O' for x in test_data.Tag ]\n",
    "# train data\n",
    "train_data = pd.read_csv('data/step_1/train_ner_dataset.csv')\n",
    "new_tags = ['B-geo', 'I-geo', 'B-gpe', 'I-gpe', 'B-org', 'I-org', 'B-per', 'I-per']\n",
    "train_data['Tag'] = [x if x in new_tags else 'O' for x in train_data.Tag ]\n",
    "# save as new datasets\n",
    "test_data.to_csv('data/step_1/test_ner_dataset_reduced_cats.csv', index=False)\n",
    "train_data.to_csv('data/step_1/train_ner_dataset_reduced_cats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using the BERT_geoparser Data.py module\n",
    "data_csv = r'data/step_1/train_ner_dataset_reduced_cats.csv'\n",
    "tokenizer = Tokenizer(size='base', cased=False)\n",
    "data = Data(data_path=data_csv, \n",
    "            tokenizer=tokenizer,\n",
    "            max_len=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 125)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 125,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 125, 768)     0           ['tf_bert_model_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 125, 10)      7690        ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,489,930\n",
      "Trainable params: 109,489,930\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jshin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new BERTModel object\n",
    "model = BertModel(saved_model=None, data=data)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2159/2159 [==============================] - 548s 248ms/step - loss: 0.0313 - accuracy: 0.2031 - val_loss: 0.0249 - val_accuracy: 0.2048\n",
      "Epoch 2/2\n",
      "2159/2159 [==============================] - 688s 318ms/step - loss: 0.0197 - accuracy: 0.2060 - val_loss: 0.0231 - val_accuracy: 0.2053\n"
     ]
    }
   ],
   "source": [
    "model.train(save_as='20230808_bert_model_large_reduced_cats.hdf5', n_epochs=2, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 43s 133ms/step\n"
     ]
    }
   ],
   "source": [
    "#model = BertModel(saved_model='20230808_bert_model_large.hdf5', data=data)\n",
    "y_pred, y_true = model.test('data/step_1/test_ner_dataset_reduced_cats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"O\" accuracy : 0.991\n",
      "\"O\" precision : 0.992\n",
      "\"O\" recall : 0.991\n",
      "=======================\n",
      "\"geo\" accuracy : 0.876\n",
      "\"geo\" precision : 0.859\n",
      "\"geo\" recall : 0.886\n",
      "=======================\n",
      "\"per\" accuracy : 0.887\n",
      "\"per\" precision : 0.872\n",
      "\"per\" recall : 0.918\n",
      "=======================\n",
      "\"gpe\" accuracy : 0.951\n",
      "\"gpe\" precision : 0.964\n",
      "\"gpe\" recall : 0.952\n",
      "=======================\n",
      "\"org\" accuracy : 0.703\n",
      "\"org\" precision : 0.776\n",
      "\"org\" recall : 0.719\n",
      "=======================\n",
      "macro average recall : 0.843\n",
      "macro average precision : 0.841\n",
      "micro average recall : 0.968\n",
      "micro average precision : 0.968\n"
     ]
    }
   ],
   "source": [
    "res = Results(y_true, y_pred)\n",
    "for cat in ['O', 'geo', 'per', 'gpe', 'org']:\n",
    "    print(f'\"{cat}\" accuracy : {np.round(res.categorical_accuracy(cat),3)}')\n",
    "    print(f'\"{cat}\" precision : {np.round(res.categorical_precision(cat),3)}')\n",
    "    print(f'\"{cat}\" recall : {np.round(res.categorical_recall(cat),3)}')\n",
    "    print('=======================')\n",
    "print(f'macro average recall : {np.round(res.macro_average_recall(), 3)}')\n",
    "print(f'macro average precision : {np.round(res.macro_average_precision(),3)}')\n",
    "print(f'micro average recall : {np.round(res.micro_average_recall(),3)}')\n",
    "print(f'micro average precision : {np.round(res.micro_average_precision(),3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
